<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.secret114.com/"/>
  <updated>2020-02-23T11:15:19.441Z</updated>
  <id>http://www.secret114.com/</id>
  
  <author>
    <name>LINCHUAN</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>动手学深度学习-Task06</title>
    <link href="http://www.secret114.com/2020/02/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Task06/"/>
    <id>http://www.secret114.com/2020/02/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Task06/</id>
    <published>2020-02-23T07:22:28.000Z</published>
    <updated>2020-02-23T11:15:19.441Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>任务06：批量归一化和残差网络；凸优化；梯度下降（1天）</p></blockquote><h2 id="批量归一化和残差网络"><a href="#批量归一化和残差网络" class="headerlink" title="批量归一化和残差网络"></a>批量归一化和残差网络</h2><h3 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h3><ul><li>浅层模型<ul><li>处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1</li><li>标准化处理输入数据使各个特征的分布相近</li></ul></li><li>深层模型<ul><li>利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定</li></ul></li></ul><h4 id="1-对全连接层做批量归一化"><a href="#1-对全连接层做批量归一化" class="headerlink" title="1.对全连接层做批量归一化"></a>1.对全连接层做批量归一化</h4><p>位置：全连接层中的仿射变换和激活函数之间。<br><strong>全连接：</strong>  </p><script type="math/tex; mode=display">\boldsymbol{x} = \boldsymbol{W\boldsymbol{u} + \boldsymbol{b}} \\ output =\phi(\boldsymbol{x})</script><p> <strong>批量归一化：</strong></p><script type="math/tex; mode=display">output=\phi(\text{BN}(\boldsymbol{x}))</script><script type="math/tex; mode=display">\boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)})</script><script type="math/tex; mode=display">\boldsymbol{\mu}_\mathcal{B} \leftarrow \frac{1}{m}\sum_{i = 1}^{m} \boldsymbol{x}^{(i)},</script><script type="math/tex; mode=display">\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2,</script><script type="math/tex; mode=display">\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}},</script><p>这⾥ϵ &gt; 0是个很小的常数，保证分母大于0</p><script type="math/tex; mode=display">{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot\hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}.</script><p>引入可学习参数：拉伸参数γ和偏移参数β。若$\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}<em>\mathcal{B}^2 + \epsilon}$和$\boldsymbol{\beta} = \boldsymbol{\mu}</em>\mathcal{B}$，批量归一化无效。</p><h4 id="2-对卷积层做批量归⼀化"><a href="#2-对卷积层做批量归⼀化" class="headerlink" title="2.对卷积层做批量归⼀化"></a>2.对卷积层做批量归⼀化</h4><p>位置：卷积计算之后、应⽤激活函数之前。<br>如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数。<br>计算：对单通道，batchsize=m,卷积计算输出=pxq<br>对该通道中m×p×q个元素同时做批量归一化,使用相同的均值和方差。</p><h4 id="3-预测时的批量归⼀化"><a href="#3-预测时的批量归⼀化" class="headerlink" title="3.预测时的批量归⼀化"></a>3.预测时的批量归⼀化</h4><p>训练：以batch为单位,对每个batch计算均值和方差。<br>预测：用移动平均估算整个训练数据集的样本均值和方差。</p><h4 id="从零实现"><a href="#从零实现" class="headerlink" title="从零实现"></a>从零实现</h4><h4 id="基于LeNet的应用"><a href="#基于LeNet的应用" class="headerlink" title="基于LeNet的应用"></a>基于LeNet的应用</h4><h4 id="简介实现"><a href="#简介实现" class="headerlink" title="简介实现"></a>简介实现</h4><h3 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h3><p>深度学习的问题：深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，准确率也变得更差。</p><h4 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h4><p>恒等映射：<br>左边：f(x)=x<br>右边：f(x)-x=0 （易于捕捉恒等映射的细微波动）<br><img src="https://cdn.kesci.com/upload/image/q5l8lhnot4.png" alt=""><br>在残差块中，输⼊可通过跨层的数据线路更快 地向前传播。</p><h4 id="ResNet模型"><a href="#ResNet模型" class="headerlink" title="ResNet模型"></a>ResNet模型</h4><p>卷积(64,7x7,3)<br>批量一体化<br>最大池化(3x3,2)</p><p>残差块x4 (通过步幅为2的残差块在每个模块之间减小高和宽)</p><p>全局平均池化</p><p>全连接</p><h2 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h2><blockquote><p>优化学习和深度学习在某种程度上来说是不太一样的，深度学习的数据集分为训练集和测试集，作为优化方法，它只能看到训练集上的数据，因此<strong>优化的数据集是训练集</strong>，而深度学习是为了测试集上的参数。</p><ul><li>优化方法目标：训练集损失函数值</li><li>深度学习目标：测试集损失函数值（泛化性）</li></ul></blockquote><p><img src="https://cdn.kesci.com/rt_upload/9349E70A9A0B46F487C86AF9A00D3002/q5p1hvo1y3.svg" alt=""></p><h3 id="优化在深度学习中的挑战"><a href="#优化在深度学习中的挑战" class="headerlink" title="优化在深度学习中的挑战"></a>优化在深度学习中的挑战</h3><h4 id="1-局部最小值"><a href="#1-局部最小值" class="headerlink" title="1.局部最小值"></a>1.局部最小值</h4><script type="math/tex; mode=display">f(x) = x\cos \pi x</script><p><img src="https://cdn.kesci.com/rt_upload/4965DB27A9A347E58616D03D993E961F/q5p1i9it7u.svg" alt="局部最小值点"></p><h4 id="2-鞍点"><a href="#2-鞍点" class="headerlink" title="2.鞍点"></a>2.鞍点</h4><p><img src="https://cdn.kesci.com/rt_upload/02890A049EE14E1D91FD5198DEDA3FFD/q5p1inxfx6.svg" alt="鞍点"></p><script type="math/tex; mode=display">A=\left[\begin{array}{cccc}{\frac{\partial^{2} f}{\partial x_{1}^{2}}} & {\frac{\partial^{2} f}{\partial x_{1} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f}{\partial x_{1} \partial x_{n}}} \\ {\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}}} & {\frac{\partial^{2} f}{\partial x_{2}^{2}}} & {\cdots} & {\frac{\partial^{2} f}{\partial x_{2} \partial x_{n}}} \\ {\vdots} & {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}}} & {\frac{\partial^{2} f}{\partial x_{n} \partial x_{2}}} & {\cdots} & {\frac{\partial^{2} f}{\partial x_{n}^{2}}}\end{array}\right]</script><p>海森矩阵结论：<br><img src="https://cdn.kesci.com/rt_upload/974B5BA9119844BD95F3E6DE8FECFE15/q5p1j9cqca.svg" alt=""></p><h4 id="3-梯度消失"><a href="#3-梯度消失" class="headerlink" title="3.梯度消失"></a>3.梯度消失</h4><p><img src="https://cdn.kesci.com/rt_upload/FD4109A263F4455EAD8C67C5DC37C027/q5p1jlrkib.svg" alt="梯度消失"></p><h3 id="凸性"><a href="#凸性" class="headerlink" title="凸性"></a>凸性</h3><h4 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h4><ul><li>图2：凸集合的交集还是凸集合</li><li>图3：两个凸集合的并不一定是凸集合</li></ul><h4 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h4><p>定义：</p><script type="math/tex; mode=display">\lambda f(x)+(1-\lambda) f\left(x^{\prime}\right) \geq f\left(\lambda x+(1-\lambda) x^{\prime}\right)</script><p><img src="https://cdn.kesci.com/rt_upload/507C2126C2654EAC8A2C220434232A3F/q5p1tqgzh5.svg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;任务06：批量归一化和残差网络；凸优化；梯度下降（1天）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;批量归一化和残差网络&quot;&gt;&lt;a href=&quot;#批量归一化和残差网络&quot; class=&quot;headerlink&quot; title=&quot;批量归一化和残差网
      
    
    </summary>
    
    
    
      <category term="Pytorch" scheme="http://www.secret114.com/tags/Pytorch/"/>
    
      <category term="DataWhale" scheme="http://www.secret114.com/tags/DataWhale/"/>
    
      <category term="Deep Learning" scheme="http://www.secret114.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习-Task05</title>
    <link href="http://www.secret114.com/2020/02/18/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Task05/"/>
    <id>http://www.secret114.com/2020/02/18/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Task05/</id>
    <published>2020-02-18T10:12:15.000Z</published>
    <updated>2020-02-19T05:06:49.337Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>任务05：卷积神经网络基础；leNet；卷积神经网络进阶（1天）</p></blockquote><h2 id="卷积神经网络基础"><a href="#卷积神经网络基础" class="headerlink" title="卷积神经网络基础"></a>卷积神经网络基础</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;任务05：卷积神经网络基础；leNet；卷积神经网络进阶（1天）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;卷积神经网络基础&quot;&gt;&lt;a href=&quot;#卷积神经网络基础&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络基础&quot;
      
    
    </summary>
    
    
    
      <category term="Pytorch" scheme="http://www.secret114.com/tags/Pytorch/"/>
    
      <category term="DataWhale" scheme="http://www.secret114.com/tags/DataWhale/"/>
    
      <category term="Deep Learning" scheme="http://www.secret114.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习-Task04</title>
    <link href="http://www.secret114.com/2020/02/18/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Task04/"/>
    <id>http://www.secret114.com/2020/02/18/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Task04/</id>
    <published>2020-02-18T09:26:33.000Z</published>
    <updated>2020-02-18T10:16:20.569Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>任务04：机器翻译及相关技术；注意力机制与Seq2seq模型；Transformer</p></blockquote><h2 id="机器翻译及相关技术"><a href="#机器翻译及相关技术" class="headerlink" title="机器翻译及相关技术"></a>机器翻译及相关技术</h2><h3 id="机器翻译和数据集"><a href="#机器翻译和数据集" class="headerlink" title="机器翻译和数据集"></a>机器翻译和数据集</h3><ol><li><p>定义：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。 </p></li><li><p>主要特征：<br>a. 输出是单词序列而不是单个单词<br>b. 输出序列的长度可能与源序列的长度不同</p></li></ol><h3 id="实战步骤讲解"><a href="#实战步骤讲解" class="headerlink" title="实战步骤讲解"></a>实战步骤讲解</h3><ul><li><p>据预处理<br>将数据集清洗、转化为神经网络的输入minbatch</p></li><li><p>分词<br>字符串&lt;==&gt;单词组成的列表</p></li><li><p>建立词典<br>单词组成的列表&lt;==&gt;单词id组成的列表</p></li><li><p>载入数据集</p></li><li><p>Encoder-Decoder</p><ul><li>encoder：输入到隐藏状态</li><li>decoder：隐藏状态到输出<br><img src="https://i.loli.net/2020/02/18/Wsl6PMEnOfb4oRG.png" alt="image.png"></li></ul></li><li><p>Sequence to Sequence模型</p><ul><li>模型</li><li>训练<br><img src="https://i.loli.net/2020/02/18/vTtQN1rszXCZOcR.png" alt="image.png"></li><li>预测<br><img src="https://i.loli.net/2020/02/18/Nrn7WFAMIJ2LcDP.png" alt="image.png"></li><li>具体结构<br><img src="https://i.loli.net/2020/02/18/h58Pd9ukwTSCIXs.png" alt="image.png"></li><li>Encoder</li><li>Decoder</li><li>损失函数</li><li>训练</li><li>测试</li></ul></li></ul><h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><p>简单greedy search：<br><img src="https://i.loli.net/2020/02/18/Lj7KTHzqGo62d34.png" alt="image.png"><br>维特比算法：选择整体分数最高的句子（搜索空间太大） 集束搜索：<br><img src="https://i.loli.net/2020/02/18/vzFeatPJo4YHuXV.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;任务04：机器翻译及相关技术；注意力机制与Seq2seq模型；Transformer&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;机器翻译及相关技术&quot;&gt;&lt;a href=&quot;#机器翻译及相关技术&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
    
      <category term="Pytorch" scheme="http://www.secret114.com/tags/Pytorch/"/>
    
      <category term="DataWhale" scheme="http://www.secret114.com/tags/DataWhale/"/>
    
      <category term="Deep Learning" scheme="http://www.secret114.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习-Task03</title>
    <link href="http://www.secret114.com/2020/02/15/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Task03/"/>
    <id>http://www.secret114.com/2020/02/15/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Task03/</id>
    <published>2020-02-15T12:53:32.000Z</published>
    <updated>2020-02-18T10:17:06.158Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>任务03：过拟合、欠拟合及其解决方案；梯度消失、梯度爆炸；循环神经网络进阶（1天）</p></blockquote><h2 id="过拟合、欠拟合及其解决方案"><a href="#过拟合、欠拟合及其解决方案" class="headerlink" title="过拟合、欠拟合及其解决方案"></a>过拟合、欠拟合及其解决方案</h2><h2 id="梯度消失、梯度爆炸"><a href="#梯度消失、梯度爆炸" class="headerlink" title="梯度消失、梯度爆炸"></a>梯度消失、梯度爆炸</h2><p>这节主要了解梯度消失、梯度爆炸两个概念，并且对这两个问题在实际应用中的发生和解决进行讲解。</p><h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。</p><h4 id="消失和爆炸"><a href="#消失和爆炸" class="headerlink" title="消失和爆炸"></a>消失和爆炸</h4><p><strong>当神经网络的层数较多时，模型的数值稳定性容易变差。</strong></p><p>假设一个层数为$L$的多层感知机的第$l$层$\boldsymbol{H}^{(l)}$的权重参数为$\boldsymbol{W}^{(l)}$，输出层$\boldsymbol{H}^{(L)}$的权重参数为$\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\phi(x) = x$。给定输入$\boldsymbol{X}$，多层感知机的第$l$层的输出$\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子：</p><blockquote><p>假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\boldsymbol{X}$分别与$0.2^{30} \approx 1 \times 10^{-21}$（消失）和$5^{30} \approx 9 \times 10^{20}$（爆炸）的乘积。</p></blockquote><p>类似的，当层数较多时，梯度的计算也容易出现消失或爆炸。</p><h4 id="随机初始化模型参数"><a href="#随机初始化模型参数" class="headerlink" title="随机初始化模型参数"></a>随机初始化模型参数</h4><p>在神经网络中，通常需要随机初始化模型参数。回顾<a href=""><s>多层感知机(占字符位)</s></a>一节描述的多层感知机。为了方便解释，假设</p><blockquote><ul><li>输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头）</li><li>隐藏层使用相同的激活函数</li></ul><p>可以看到如下图的变化：<br><img src="https://i.loli.net/2020/02/18/GZ79OPcjw1aoIJB.png" alt="假设模型"></p><p>如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。</p></blockquote><p>在这种情况下，无论隐藏单元有多少，隐藏层<strong>本质上只有1个隐藏单元在发挥作用</strong>。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p><h4 id="Pytorch的默认随机初始化"><a href="#Pytorch的默认随机初始化" class="headerlink" title="Pytorch的默认随机初始化"></a>Pytorch的默认随机初始化</h4><p>随机初始化模型参数的方法有很多。在<a href="">线性回归的简洁实现</a>中，我们使用<code>torch.nn.init.normal_()</code>使模型<code>net</code>的权重参数采用正态分布的随机初始化方式。</p><blockquote><p>PyTorch中<code>nn.Module</code>的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules" target="_blank" rel="noopener">源代码</a>），因此一般不用我们考虑。</p></blockquote><h4 id="Xavier随机初始化"><a href="#Xavier随机初始化" class="headerlink" title="Xavier随机初始化"></a>Xavier随机初始化</h4><p>还有一种比较常用的随机初始化方法叫作Xavier随机初始化。<br>假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p><script type="math/tex; mode=display">U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).</script><p>它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p><h3 id="考虑环境因素"><a href="#考虑环境因素" class="headerlink" title="考虑环境因素"></a>考虑环境因素</h3><h4 id="协变量偏移"><a href="#协变量偏移" class="headerlink" title="协变量偏移"></a>协变量偏移</h4><p>这里我们假设，虽然输入的分布可能随时间而改变，但是标记函数，即条件分布$P\left( y∣x \right)$不会改变。虽然这个问题容易理解，但在实践中也容易忽视。</p><p>想想区分猫和狗的一个例子。我们的训练数据使用的是猫和狗的真实的照片，但是在测试时，我们被要求对猫和狗的卡通图片进行分类。</p><ul><li>训练数据</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">cat</th><th style="text-align:center">cat</th><th style="text-align:center">dog</th><th style="text-align:center">dog</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://i.loli.net/2020/02/17/tvnQlmC2cJBKdX5.jpg" alt="cat" style="zoom:20%;" /></td><td style="text-align:center"><img src="https://i.loli.net/2020/02/17/T9pdasx4AymvkDL.jpg" alt="cat" style="zoom:20%;" /></td><td style="text-align:center"><img src="https://i.loli.net/2020/02/17/R9pLxyzbsUQ4tGM.jpg" alt="q5jg9tqs4s.jpg" style="zoom:20%;" /></td><td style="text-align:center"><img src="https://i.loli.net/2020/02/17/uiESAmbYQU12XDZ.jpg" alt="q5jga6mnsk.jpg" style="zoom:20%;" /></td></tr></tbody></table></div><ul><li>测试数据</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">cat</th><th style="text-align:center">cat</th><th style="text-align:center">dog</th><th style="text-align:center">dog</th></tr></thead><tbody><tr><td style="text-align:center"><img src="https://i.loli.net/2020/02/17/HWxviMCyZNd7z34.png" alt="cat" style="zoom:20%;" /></td><td style="text-align:center"><img src="https://i.loli.net/2020/02/17/4OcuKG1CqwStBvA.png" alt="cat" style="zoom:20%;" /></td><td style="text-align:center"><img src="https://i.loli.net/2020/02/17/e2nARVqKpzTs5Uu.png" alt="q5jg9tqs4s.jpg" style="zoom:20%;" /></td><td style="text-align:center"><img src="https://i.loli.net/2020/02/17/mfeNkuJGZKL2EIc.png" alt="q5jga6mnsk.jpg" style="zoom:20%;" /></td></tr></tbody></table></div><p>显然，这不太可能奏效。训练集由照片组成，而测试集只包含卡通。在一个看起来与测试集有着本质不同的数据集上进行训练，而不考虑如何适应新的情况，这是不是一个好主意。不幸的是，这是一个非常常见的陷阱。</p><p>统计学家称这种协变量变化是因为问题的根源在于特征分布的变化（即协变量的变化）。数学上，我们可以说$P（x）$改变了，但$P（y∣x）$保持不变。尽管它的有用性并不局限于此，当我们认为$x$导致$y$时，协变量移位通常是正确的假设。</p><h4 id="标签偏移"><a href="#标签偏移" class="headerlink" title="标签偏移"></a>标签偏移</h4><p>当我们认为导致偏移的是标签$P（y）$上的边缘分布的变化，但类条件分布是不变的$P（x∣y）$时，就会出现相反的问题。当我们认为$y$导致$x$时，标签偏移是一个合理的假设。例如，通常我们希望根据其表现来预测诊断结果。在这种情况下，我们认为诊断引起的表现，即疾病引起的症状。有时标签偏移和协变量移位假设可以同时成立。例如，当真正的标签函数是确定的和不变的，那么协变量偏移将始终保持，包括如果标签偏移也保持。有趣的是，当我们期望标签偏移和协变量偏移保持时，使用来自标签偏移假设的方法通常是有利的。这是因为这些方法倾向于操作看起来像标签的对象，这（在深度学习中）与处理看起来像输入的对象（在深度学习中）相比相对容易一些。</p><p>病因（要预测的诊断结果）导致 症状（观察到的结果）。  </p><p>训练数据集，数据很少只包含流感$p(y)$的样本。  </p><p>而测试数据集有流感$p(y)$和流感$q(y)$，其中不变的是流感症状$p(x|y)$。</p><h4 id="概念偏移"><a href="#概念偏移" class="headerlink" title="概念偏移"></a>概念偏移</h4><p>另一个相关的问题出现在概念转换中，即标签本身的定义发生变化的情况。这听起来很奇怪，毕竟猫就是猫。的确，猫的定义可能不会改变，但我们能不能对软饮料也这么说呢？事实证明，如果我们周游美国，按地理位置转移数据来源，我们会发现，即使是如图所示的这个简单术语的定义也会发生相当大的概念转变。<br><img src="https://i.loli.net/2020/02/18/JxOgzkVucRYlCqt.png" alt="美国软饮料名称的概念转变"></p><p>如果我们要建立一个机器翻译系统，分布P（y∣x）可能因我们的位置而异。这个问题很难发现。另一个可取之处是P（y∣x）通常只是逐渐变化。</p><h2 id="Kaggle房价预测实战（暂略）"><a href="#Kaggle房价预测实战（暂略）" class="headerlink" title="Kaggle房价预测实战（暂略）"></a>Kaggle房价预测实战（暂略）</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;任务03：过拟合、欠拟合及其解决方案；梯度消失、梯度爆炸；循环神经网络进阶（1天）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;过拟合、欠拟合及其解决方案&quot;&gt;&lt;a href=&quot;#过拟合、欠拟合及其解决方案&quot; class=&quot;headerlin
      
    
    </summary>
    
    
    
      <category term="Pytorch" scheme="http://www.secret114.com/tags/Pytorch/"/>
    
      <category term="DataWhale" scheme="http://www.secret114.com/tags/DataWhale/"/>
    
      <category term="Deep Learning" scheme="http://www.secret114.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Task02：文本预处理；语言模型；循环神经网络基础</title>
    <link href="http://www.secret114.com/2020/02/14/Task02%EF%BC%9A%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%9B%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>http://www.secret114.com/2020/02/14/Task02%EF%BC%9A%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%9B%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</id>
    <published>2020-02-14T11:21:58.000Z</published>
    <updated>2020-02-14T11:21:58.178Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>动手学深度学习-Task01</title>
    <link href="http://www.secret114.com/2020/02/13/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Task01/"/>
    <id>http://www.secret114.com/2020/02/13/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Task01/</id>
    <published>2020-02-13T03:08:06.000Z</published>
    <updated>2020-02-18T10:17:09.482Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Softmax和分类模型"><a href="#Softmax和分类模型" class="headerlink" title="Softmax和分类模型"></a>Softmax和分类模型</h2><h3 id="Softmax基本概念"><a href="#Softmax基本概念" class="headerlink" title="Softmax基本概念"></a>Softmax基本概念</h3><p>抛砖引玉，对于一个简单的图像分类问题，假设输入图像的high和width均为2个像素，忽略色彩方面因素（原文设置为灰度），则图像的4个像素可以设置为$x_1$，$x_2$，$x_3$，$x_4$。假如此时，有3个真实标签$y_1=1$，$y_2=2$，$y_3=3$分别代表鸡、狗、猪，则我们以此得到有关的权重矢量关系式：</p><script type="math/tex; mode=display">\begin{aligned}    o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}    o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2\end{aligned}</script><script type="math/tex; mode=display"> \begin{aligned} o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3 \end{aligned}</script><p>用神经网络绘制上面的计算：<br><img src="https://cdn.kesci.com/upload/image/q5hmymezog.png" alt="Image Name"><br>Softmax回归同线性回归一样也是<strong>单层神经网络</strong>结构，输出层是一个全连接层。</p><p>回到上面举的例子，既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，并将值最大的输出所对应的类作为预测输出，即输出 $\underset{i}{\arg\max} o_i$。例如，如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表狗。</p><h4 id="输出问题"><a href="#输出问题" class="headerlink" title="输出问题"></a>输出问题</h4><p>直接使用输出层的输出有两个问题：</p><ol><li>一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果$o_1=o_3=10^3$，那么输出值10却又表示图像类别为猫的概率很低。</li><li>另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</li></ol><p>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：</p><script type="math/tex; mode=display"> \hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)</script><p>其中</p><script type="math/tex; mode=display"> \hat{y}1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y}3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.</script><h4 id="计算效率"><a href="#计算效率" class="headerlink" title="计算效率"></a>计算效率</h4><h3 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h3><p>对于样本$i$，我们构造向量$\boldsymbol{y}^{(i)}\in \mathbb{R}^{q}$ ，使其第$y^{(i)}$（样本$i$类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布$\boldsymbol{\hat y}^{(i)}$尽可能接近真实的标签概率分布$\boldsymbol{y}^{(i)}$。</p><ul><li>平方损失估计<script type="math/tex; mode=display">\begin{aligned}Loss = |\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}|^2/2\end{aligned}</script></li></ul><p>然而，在实际应用中，我们其实并不需要预测标签完全等于真实标签，例如$y^{(i)}=3$，我们只需要$\hat{y}^{(i)}_3$比其他两个预测值$\hat{y}^{(i)}_1$和$\hat{y}^{(i)}_2$大就行了，即使$\hat{y}^{(i)}_3$值为0.6，不管其他两个预测值为多少，类别预测均正确。<br>因此，为了改善上述问题，我们采用交叉熵（Cross Entropy）：</p><script type="math/tex; mode=display">H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},</script><p>其中带下标的$y_j^{(i)}$是向量$\boldsymbol y^{(i)}$中非0即1的元素。</p><p>假设训练数据集的样本数为$n$，交叉熵损失函数定义为 </p><script type="math/tex; mode=display">\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),</script><p>其中$\boldsymbol{\Theta}$代表模型参数。</p><h3 id="获取Fashion-MNIST训练集和读取数据"><a href="#获取Fashion-MNIST训练集和读取数据" class="headerlink" title="获取Fashion-MNIST训练集和读取数据"></a>获取Fashion-MNIST训练集和读取数据</h3><p>图像分类数据集中最常用的是手写数字识别数据集MNIST[1]。但大部分模型在MNIST上的分类精度都超过了95%。为了更直观地观察算法之间的差异，我们将使用一个图像内容更加复杂的数据集Fashion-MNIST[2]。</p><p>我这里我们会使用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：</p><ol><li>torchvision.datasets: 一些加载数据的函数及常用的数据集接口；</li><li>torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；</li><li>torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；</li><li>torchvision.utils: 其他的一些有用的方法。</li></ol><p><s>代码请见jupyter notebook，后续补充</s><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torchvision</span>.<span class="title">datasets</span>.<span class="title">FashionMNIST</span><span class="params">(root, train=True, transform=None, target_transform=None, download=False)</span></span></span><br></pre></td></tr></table></figure></p><ul><li>root（string）– 数据集的根目录，其中存放processed/training.pt和processed/test.pt文件。<ul><li>train（bool, 可选）– 如果设置为True，从training.pt创建数据集，否则从test.pt创建。</li><li>download（bool, 可选）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。</li><li>transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。</li><li>target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。</li></ul></li></ul><h3 id="Softmax从零开始"><a href="#Softmax从零开始" class="headerlink" title="Softmax从零开始"></a>Softmax从零开始</h3><h3 id="Softmax的简洁实现"><a href="#Softmax的简洁实现" class="headerlink" title="Softmax的简洁实现"></a>Softmax的简洁实现</h3><h2 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h2><ol><li><p>softmax([100, 101, 102])的结果等于以下的哪一项（）<br>A. softmax([10.0, 10.1, 10.2])<br>B. softmax([-100, -101, -102])<br>C. softmax([-2 -1, 0])<br>D. softmax([1000, 1010, 1020])</p></li><li><p>对于本节课的模型，在刚开始训练时，训练数据集上的准确率低于测试数据集上的准确率，原因是（）<br>A. 模型参数是在训练集上进行训练的，可能陷入了过拟合<br>B. 训练集的样本容量更大，要提高准确率更难<br>C. 训练集上的准确率是在一个epoch的过程中计算得到的，测试集上的准确率是在一个epoch结束后计算得到的，后者的模型参数更优</p></li></ol><h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><ol><li>隐藏层</li></ol><p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\boldsymbol{H}$，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p><p><img src="https://cdn.kesci.com/upload/image/q5ho684jmh.png" alt="多层感知机的神经网络图"></p><p>我们先来看一种含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为</p><script type="math/tex; mode=display"> \begin{aligned} \boldsymbol{H} &= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\\ \boldsymbol{O} &= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}</script><p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到</p><script type="math/tex; mode=display"> \boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.</script><p>从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><h4 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h4><p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为</p><script type="math/tex; mode=display">\text{ReLU}(x) = \max(x, 0).</script><p>可以看出，ReLU函数只保留正数元素，并将负数元素清零。为了直观地观察这一非线性变换，我们先定义一个绘图函数xyplot。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="comment"># sys.path.append("/home/kesci/input")</span></span><br><span class="line"><span class="comment"># import d2lzh1981 as d2l</span></span><br><span class="line"><span class="comment"># print(torch.__version__)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xyplot</span><span class="params">(x_vals, y_vals, name)</span>:</span></span><br><span class="line">    <span class="comment"># d2l.set_figsize(figsize=(5, 2.5))</span></span><br><span class="line">    plt.plot(x_vals.detach().numpy(), y_vals.detach().numpy())</span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">    plt.ylabel(name + <span class="string">'(x)'</span>)</span><br><span class="line">x = torch.arange(<span class="number">-8.0</span>, <span class="number">8.0</span>, <span class="number">0.1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x.relu()</span><br><span class="line">xyplot(x, y, <span class="string">'relu'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.kesci.com/rt_upload/070825B6A382411DA5BD7D14E67E8D54/q5hv7cdtna.png" alt=""></p><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x.sigmoid()</span><br><span class="line">xyplot(x, y, <span class="string">'sigmoid'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.kesci.com/rt_upload/68FCB4E8142144458F13128B370D1C91/q5hv7dor11.png" alt=""></p><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x.tanh()</span><br><span class="line">xyplot(x, y, <span class="string">'tanh'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.kesci.com/rt_upload/92D16076309F42169482834C0B6ABB24/q5hv7dfeso.png" alt=""></p><h4 id="关于激活函数的选择"><a href="#关于激活函数的选择" class="headerlink" title="关于激活函数的选择"></a>关于激活函数的选择</h4><p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p><p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。</p><p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p><p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Softmax和分类模型&quot;&gt;&lt;a href=&quot;#Softmax和分类模型&quot; class=&quot;headerlink&quot; title=&quot;Softmax和分类模型&quot;&gt;&lt;/a&gt;Softmax和分类模型&lt;/h2&gt;&lt;h3 id=&quot;Softmax基本概念&quot;&gt;&lt;a href=&quot;#So
      
    
    </summary>
    
    
    
      <category term="Pytorch" scheme="http://www.secret114.com/tags/Pytorch/"/>
    
      <category term="DataWhale" scheme="http://www.secret114.com/tags/DataWhale/"/>
    
      <category term="Deep Learning" scheme="http://www.secret114.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>[Leetcode]二叉树的前/中/后序和层次遍历（94/102/144/145）</title>
    <link href="http://www.secret114.com/2020/02/11/Leetcode-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%89%8D-%E4%B8%AD-%E5%90%8E%E5%BA%8F%E5%92%8C%E5%B1%82%E6%AC%A1%E9%81%8D%E5%8E%86%EF%BC%8894-102-144-145%EF%BC%89/"/>
    <id>http://www.secret114.com/2020/02/11/Leetcode-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%89%8D-%E4%B8%AD-%E5%90%8E%E5%BA%8F%E5%92%8C%E5%B1%82%E6%AC%A1%E9%81%8D%E5%8E%86%EF%BC%8894-102-144-145%EF%BC%89/</id>
    <published>2020-02-11T07:12:50.000Z</published>
    <updated>2020-02-11T15:47:44.882Z</updated>
    
    <content type="html"><![CDATA[<!-- @import "[TOC]" {cmd="toc" depthFrom=2 depthTo=3 orderedList=false} --><!-- code_chunk_output --><ul><li><a href="#94-二叉树的中序遍历">94. 二叉树的中序遍历</a><ul><li><a href="#方法一遍历">方法一：遍历</a></li><li><a href="#方法二迭代">方法二：迭代</a></li></ul></li><li><a href="#144二叉树的前序遍历">144.二叉树的前序遍历</a><ul><li><a href="#方法一遍历-1">方法一：遍历</a></li><li><a href="#方法二迭代-1">方法二：迭代</a></li></ul></li></ul><!-- /code_chunk_output --><p>这篇主要是针对二叉树的遍历进行代码详解，目的是学会二叉树各类遍历的实际应用代码模板，学以致用。文章内容按照Leetcode上题号顺序分类解答，最终列出相关通用模板和方法。</p><h2 id="94-二叉树的中序遍历"><a href="#94-二叉树的中序遍历" class="headerlink" title="94. 二叉树的中序遍历"></a>94. 二叉树的中序遍历</h2><blockquote><p>规律：<strong>左 - 根 - 右</strong></p></blockquote><h3 id="方法一：遍历"><a href="#方法一：遍历" class="headerlink" title="方法一：遍历"></a>方法一：遍历</h3><p>根据二叉树的中序遍历情况，递归只需要根据规律将结果装入<code>res</code>中即可，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inorderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = [] <span class="comment"># 存储最终结果的变量</span></span><br><span class="line">        self.helper(root,res) <span class="comment"># helper函数用于递归</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self,root,res)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        函数功能：按照“左根右”递归遍历二叉树</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :type res: List[int]</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> ; <span class="comment"># 若二叉树为空，则直接返回</span></span><br><span class="line">        self.helper(root.left,res) <span class="comment"># 遍历左子树</span></span><br><span class="line">        res.append(root.val) <span class="comment"># 将根节点放入res中</span></span><br><span class="line">        self.helper(root.right,res) <span class="comment"># 遍历右子树</span></span><br></pre></td></tr></table></figure><h3 id="方法二：迭代"><a href="#方法二：迭代" class="headerlink" title="方法二：迭代"></a>方法二：迭代</h3><p>上面的方法容易造成“一看就会，一用就跪”的局面，并且对于计算机来说递归次数越多代码运行所消耗的内存和时间就越大，因此，在实际应用中，常采用迭代来替代递归。</p><p>迭代方法的关键在于以空间换时间，<strong>用栈来存储遍历时经过的节点</strong>（这点思路非常重要，并且要注意是栈不是队列！）。与其他遍历不同（前序、后序），中序遍历需要存储所有遍历到的左节点，找到<strong>最左侧</strong>节点（叶子节点）后分步释放节点并迭代右节点，这部分代码需要注意，其代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inorderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        stack,res = [],[] <span class="comment"># 栈stack存储遍历节点,res存储结果</span></span><br><span class="line">        cur = root <span class="comment"># 存储当前节点</span></span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">or</span> cur:</span><br><span class="line">            <span class="comment"># 遍历左子树，直到最左节点</span></span><br><span class="line">            <span class="keyword">while</span> cur:</span><br><span class="line">                stack.append(cur) </span><br><span class="line">                cur = cur.left</span><br><span class="line">            <span class="comment"># 释放左节点，并开始迭代右节点</span></span><br><span class="line">            cur = stack.pop() <span class="comment"># 当前栈顶保存了最左节点的信息</span></span><br><span class="line">            res.append(cur.val)</span><br><span class="line">            cur = cur.right <span class="comment"># 迭代右子树</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h2 id="144-二叉树的前序遍历"><a href="#144-二叉树的前序遍历" class="headerlink" title="144.二叉树的前序遍历"></a>144.二叉树的前序遍历</h2><blockquote><p>规律：<strong>根 - 左 - 右</strong></p></blockquote><h3 id="方法一：遍历-1"><a href="#方法一：遍历-1" class="headerlink" title="方法一：遍历"></a>方法一：遍历</h3><p>跟之前的遍历一样，按照规律编写如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inorderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type root: TreeNode</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = []</span><br><span class="line">        self.helper(root,res)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self,root,res)</span></span></span><br><span class="line"><span class="function">        <span class="title">if</span> <span class="title">not</span> <span class="title">root</span>:</span></span><br><span class="line">            <span class="keyword">return</span> ;</span><br><span class="line">        res.append(root.val) <span class="comment"># 将根节点写入res中</span></span><br><span class="line">        self.helper(root.left,res) <span class="comment"># 遍历左子树</span></span><br><span class="line">        self.helper(root.right,res) <span class="comment"># 遍历右子树</span></span><br></pre></td></tr></table></figure><h3 id="方法二：迭代-1"><a href="#方法二：迭代-1" class="headerlink" title="方法二：迭代"></a>方法二：迭代</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- @import &quot;[TOC]&quot; {cmd=&quot;toc&quot; depthFrom=2 depthTo=3 orderedList=false} --&gt;

&lt;!-- code_chunk_output --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#94-二叉树的中序遍历&quot;&gt;94
      
    
    </summary>
    
    
    
      <category term="Leetcode" scheme="http://www.secret114.com/tags/Leetcode/"/>
    
      <category term="Python" scheme="http://www.secret114.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>[Leetcode]76.最小覆盖子串</title>
    <link href="http://www.secret114.com/2020/01/20/Leetcode-76-%E6%9C%80%E5%B0%8F%E8%A6%86%E7%9B%96%E5%AD%90%E4%B8%B2/"/>
    <id>http://www.secret114.com/2020/01/20/Leetcode-76-%E6%9C%80%E5%B0%8F%E8%A6%86%E7%9B%96%E5%AD%90%E4%B8%B2/</id>
    <published>2020-01-20T06:56:01.000Z</published>
    <updated>2020-02-10T07:04:02.300Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>题号：76</p><p>题名：最小覆盖子串</p><p>难度：H</p><p>地址：<a href="https://leetcode-cn.com/problems/minimum-window-substring/submissions/" target="_blank" rel="noopener">https://leetcode-cn.com/problems/minimum-window-substring/submissions/</a></p></blockquote><h1 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h1><p>参照<a href="https://zhuanlan.zhihu.com/p/67687874" target="_blank" rel="noopener" title="【知乎】[LeetCode] 76. 最小覆盖子串">知乎</a>的解法，熟悉滑动窗口</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;题号：76&lt;/p&gt;
&lt;p&gt;题名：最小覆盖子串&lt;/p&gt;
&lt;p&gt;难度：H&lt;/p&gt;
&lt;p&gt;地址：&lt;a href=&quot;https://leetcode-cn.com/problems/minimum-window-substring/submissions/
      
    
    </summary>
    
    
    
      <category term="Leetcode" scheme="http://www.secret114.com/tags/Leetcode/"/>
    
      <category term="Python" scheme="http://www.secret114.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>关于windows控制台或Cmder打开时提示“系统找不到指定的路径”问题</title>
    <link href="http://www.secret114.com/2020/01/08/%E5%85%B3%E4%BA%8Ewindows%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%88%96Cmder%E6%89%93%E5%BC%80%E6%97%B6%E6%8F%90%E7%A4%BA%E2%80%9C%E7%B3%BB%E7%BB%9F%E6%89%BE%E4%B8%8D%E5%88%B0%E6%8C%87%E5%AE%9A%E7%9A%84%E8%B7%AF%E5%BE%84%E2%80%9D%E9%97%AE%E9%A2%98/"/>
    <id>http://www.secret114.com/2020/01/08/%E5%85%B3%E4%BA%8Ewindows%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%88%96Cmder%E6%89%93%E5%BC%80%E6%97%B6%E6%8F%90%E7%A4%BA%E2%80%9C%E7%B3%BB%E7%BB%9F%E6%89%BE%E4%B8%8D%E5%88%B0%E6%8C%87%E5%AE%9A%E7%9A%84%E8%B7%AF%E5%BE%84%E2%80%9D%E9%97%AE%E9%A2%98/</id>
    <published>2020-01-08T00:56:48.000Z</published>
    <updated>2020-01-08T01:07:48.087Z</updated>
    
    <content type="html"><![CDATA[<p>昨天因为心血来潮，突然删了用户根目录下的配置文件夹（主要是觉得有些环境配置用不到了就顺带一起清理下），结果打开cmder时一连提示好几个“系统找不到指定的路径”，删了cmder重配置也是一样。后来我发现cmd下也会提示这个错误，于是在网上搜了下，发现了这个解决办法。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>首先<code>regedit</code>打开注册表，找到<code>计算机\HKEY_CURRENT_USER\Software\Microsoft\Command Processor</code>路径下<code>Autorun</code>注册表项，删除即可。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;昨天因为心血来潮，突然删了用户根目录下的配置文件夹（主要是觉得有些环境配置用不到了就顺带一起清理下），结果打开cmder时一连提示好几个“系统找不到指定的路径”，删了cmder重配置也是一样。后来我发现cmd下也会提示这个错误，于是在网上搜了下，发现了这个解决办法。&lt;/p&gt;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>1. Two Sum-两数之和</title>
    <link href="http://www.secret114.com/2020/01/05/1-Two-Sum-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/"/>
    <id>http://www.secret114.com/2020/01/05/1-Two-Sum-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/</id>
    <published>2020-01-05T09:52:17.000Z</published>
    <updated>2020-01-05T09:54:52.056Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Pandas-API-DataFrame-用法</title>
    <link href="http://www.secret114.com/2020/01/05/Pandas-API-DataFrame-%E7%94%A8%E6%B3%95/"/>
    <id>http://www.secret114.com/2020/01/05/Pandas-API-DataFrame-%E7%94%A8%E6%B3%95/</id>
    <published>2020-01-05T04:37:16.273Z</published>
    <updated>2019-07-02T07:11:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>title: ‘[Pandas API]DataFrame()用法’<br>date: 2019-07-02 11:58:14<br>tags: [api]</p><p>API地址：<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" target="_blank" rel="noopener">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html</a></p><h1 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h1><p><code>DataFrame</code>是Python中<code>Pandas</code>库中的一种数据结构，它类似于Excel，是一种<strong>二维表</strong>。<code>DataFrame</code>可以存放多类型数据，例如数值、字符串等。</p><h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.DataFrame([data,index,columns,dtype,copy]) <span class="comment"># pandas这里简写为pd</span></span><br></pre></td></tr></table></figure><ul><li>data: 可以是<code>ndarray</code>、<code>lterable</code>、<code>DataFrame</code>类型</li><li>index: 用于生成结果索引</li><li>dtype: 强制输出一个</li><li>copy: 从输入中复制数据</li></ul><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>Pandas库经常与Numpy库连用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;title: ‘[Pandas API]DataFrame()用法’&lt;br&gt;date: 2019-07-02 11:58:14&lt;br&gt;tags: [api]&lt;/p&gt;
&lt;p&gt;API地址：&lt;a href=&quot;https://pandas.pydata.org/pandas-doc
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Anaconda下配置多版本环境</title>
    <link href="http://www.secret114.com/2019/07/13/Anaconda%E4%B8%8B%E9%85%8D%E7%BD%AE%E5%A4%9A%E7%89%88%E6%9C%AC%E7%8E%AF%E5%A2%83/"/>
    <id>http://www.secret114.com/2019/07/13/Anaconda%E4%B8%8B%E9%85%8D%E7%BD%AE%E5%A4%9A%E7%89%88%E6%9C%AC%E7%8E%AF%E5%A2%83/</id>
    <published>2019-07-12T22:40:10.000Z</published>
    <updated>2019-07-14T03:18:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>文章详述了有关Anaconda软件包中关于conda配置多版本环境的内容，多环境用于不同编程需要。<br>核心命令为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create [--name | -a] &lt;envsname&gt; [python=x.x]</span><br><span class="line">conda activate &lt;envsname&gt;</span><br></pre></td></tr></table></figure><p><img src="http://image.secret114.com/Image/Anaconda-open-source-software.png" alt="Anaconda"></p><a id="more"></a><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Anaconda是一款开源的Python发行版本，其包含了conda、python等许多科学包和依赖环境，我平时学习时最常使用的Jupter Notebook就是运行在Anaconda软件环境中的，平时最常用的就是下载包和看文档比较方便，没有深入研究过这个工具的强大作用。这次利用pytorch中的torchtext做情感分析实验发现，pytorch倒是很容易安装上了（这里吐槽Pycharm各种报错，难受），而torchtext所需Python版本是3.7，安装时报出各种安装依赖。后来，在简书上找到一篇<a href="https://www.jianshu.com/p/a52c8b514ea9" target="_blank" rel="noopener" title="conda下多版本Python切换和配置">《conda下多版本Python切换和配置》</a>，试着做了一些配置，发现确实好用，又get到一种新方法～</p><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>Anaconda环境下需要多个Python版本之间的切换，原始的Anaconda下的Python版本无法满足要求（例如，两个notebook运行环境需要在python 2.7.x和python 3.6.x之间进行切换，总不能要用哪个notebook时先把另一个notebook环境关了吧？这样切换太麻烦了）</p><h2 id="相关环境"><a href="#相关环境" class="headerlink" title="相关环境"></a>相关环境</h2><p>我这边是运行在Vmware虚拟机里的UBuntu，也就是linux环境，系统Python默认版本为3.6，也带有Python 2.7、3.7。总结如下：</p><ul><li><p>OS: Ubuntu(Linux)</p></li><li><p>Python: 3.6</p></li><li><p>conda: 4.7.5</p></li></ul><h1 id="解决步骤"><a href="#解决步骤" class="headerlink" title="解决步骤"></a>解决步骤</h1><ol><li>首先检测安装的conda版本，并将conda升级到最新版本：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda --version <span class="comment"># 查看conda版本</span></span><br></pre></td></tr></table></figure><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2009-48-23.png" alt="查看conda版本"></p><p>如果版本较低可以升级conda，或者顺手升级Anaconda环境的所有依赖包版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda update conda <span class="comment"># 升级conda</span></span><br><span class="line">conda update --all <span class="comment"># 升级Anaconda所有依赖包</span></span><br></pre></td></tr></table></figure><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2009-58-29.png" alt="升级conda"></p><p>这里我省略了安装过程的刷屏代码，最后只要看见<code>All requested packages already installed.</code>就代表成功了。</p><blockquote><p>这里注意，如果出现Anaconda安装了但是conda命令无法执行，那可能是环境变量没有添加。一般bash没问题，这里我用的是zsh，需要手动添加环境变量：</p><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2009-41-30.png" alt="zsh下conda命令无法找到"></p><p>在<code>~/.zshrc</code>文件末尾添加一句：</p><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2009-45-07.png" alt="配置zsh环境变量"></p><p>最后<code>source ~/.zshrc</code>即可，这样就能正常使用conda命令了。</p></blockquote><ol start="2"><li>列出所有conda环境</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></table></figure><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2010-01-40.png" alt="显示所有conda环境"></p><p>这里发现codna一共两个环境（其中带*的为默认环境），使用的python版本都是3.6，而我的notebook torch需要python 3.7的相关依赖——</p><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2010-10-42.png" alt="安装torchtext时提示python相关package版本不匹配"></p><p>所以我们需要创建一个新的Python 3.7环境安装torchtext。</p><ol start="3"><li>创建一个新环境并激活</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n py37  [python=3.7]</span><br></pre></td></tr></table></figure><p>其中，<code>py37</code>为创建的环境名称，后面指定了Python版本，也可以不添加，就是分配默认版本。</p><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2018-13-26.png" alt="创建conda环境"></p><p>选择<code>y</code>自动配置完成后，会出现提示：</p><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2018-14-19.png" alt="conda新环境配置完成"></p><p>我们按照提示激活环境即可使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate py37</span><br></pre></td></tr></table></figure><p>注意，在zsh命令行中输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> activate &lt;envname&gt;</span><br></pre></td></tr></table></figure><blockquote><p><s>需要在Terminal下切换conda环境的话也是使用<code>conda activate &lt;envname&gt;</code>就可以完成切换，想要回到原始环境就<code>conda deactivate</code>。而在Jupyter notebook中直接选择<code>kernel -&gt; Change kernel</code></s></p></blockquote><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2018-26-11.png" alt="激活环境 &amp; 切换环境"></p><p>可以看到，<code>conda activate</code>命令在各环境下进行切换，十分方便。</p><ol start="4"><li>其他配置</li></ol><p>conda还有许多配置命令，下面一一列举：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda list [--name &lt;envname&gt;] <span class="comment"># 检查所有package，默认为当前conda环境</span></span><br><span class="line">conda search &lt;package name&gt; <span class="comment"># 查找package信息</span></span><br><span class="line">conda install [--name &lt;envname&gt;] &lt;package name&gt; <span class="comment"># 将package安装到某个conda环境中，默认为当前环境</span></span><br><span class="line">conda remove --name &lt;envname&gt; &lt;package name | --all&gt; <span class="comment"># 移除依赖包，或者移除整个环境 </span></span><br><span class="line">conda update [--name &lt;envname&gt;] &lt;package name&gt; <span class="comment"># 更新依赖包，默认为当前conda环境</span></span><br></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.jianshu.com/p/a52c8b514ea9" target="_blank" rel="noopener" title="conda下多版本Python切换和配置">简书：conda下多版本Python切换和配置</a></li><li><a href="https://blog.csdn.net/neu_chenguangq/article/details/79451945" target="_blank" rel="noopener" title="conda-多环境配置">CSDN：conda-多环境配置</a></li><li><a href="https://zhuanlan.zhihu.com/p/46902996" target="_blank" rel="noopener" title="[工具] Anaconda 环境管理工具 如何安装不同版本的R和python">知乎：[工具] Anaconda 环境管理工具 如何安装不同版本的R和python</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;文章详述了有关Anaconda软件包中关于conda配置多版本环境的内容，多环境用于不同编程需要。&lt;br&gt;核心命令为&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;conda create [--name | -a] &amp;lt;envsname&amp;gt; [python=x.x]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conda activate &amp;lt;envsname&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;http://image.secret114.com/Image/Anaconda-open-source-software.png&quot; alt=&quot;Anaconda&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="anaconda" scheme="http://www.secret114.com/tags/anaconda/"/>
    
  </entry>
  
  <entry>
    <title>Linux下安装oh-my-zsh</title>
    <link href="http://www.secret114.com/2019/07/13/Linux%E4%B8%8B%E5%AE%89%E8%A3%85oh-my-zsh/"/>
    <id>http://www.secret114.com/2019/07/13/Linux%E4%B8%8B%E5%AE%89%E8%A3%85oh-my-zsh/</id>
    <published>2019-07-12T22:39:28.000Z</published>
    <updated>2019-07-14T03:36:35.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>注意：安装前先备份<code>/etc/passwd</code></strong></p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install zsh</span><br><span class="line">chsh -s /bin/zsh <span class="comment"># 注意，不要使用sudo</span></span><br><span class="line">sudo vim /etc/passwd</span><br></pre></td></tr></table></figure><p>修改第一行的<code>/bin/bash</code>变成<code>/bin/zsh</code>：</p><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2005-59-31.png" alt=""></p><p>把最后一行<code>/bin/bash</code>改为<code>/bin/zsh</code>：</p><p><img src="http://image.secret114.com/Screenshot%20from%202019-07-13%2006-00-16.png" alt=""></p><p>然后根据<a href="https://ohmyz.sh/" target="_blank" rel="noopener" title="oh-my-zsh官网">官网</a>提供的命令自动安装oh-my-zsh：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c <span class="string">"<span class="variable">$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>"</span></span><br></pre></td></tr></table></figure><p>然后重启电脑后就能看到oh-my-zsh安装好了，目前我没遇见过其他错误，还正在体验中</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.cnblogs.com/EasonJim/p/7863099.html" target="_blank" rel="noopener" title="Ubuntu 16.04下安装zsh和oh-my-zsh">博客园：Ubuntu 16.04下安装zsh和oh-my-zsh</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;注意：安装前先备份&lt;code&gt;/etc/passwd&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h1&gt;&lt;figure class=&quot;
      
    
    </summary>
    
    
    
      <category term="Linux" scheme="http://www.secret114.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>理解NLP中的卷积神经网络(CNN)</title>
    <link href="http://www.secret114.com/2019/07/04/%E7%90%86%E8%A7%A3NLP%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN/"/>
    <id>http://www.secret114.com/2019/07/04/%E7%90%86%E8%A7%A3NLP%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN/</id>
    <published>2019-07-04T10:02:25.000Z</published>
    <updated>2019-07-04T10:02:25.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>[TensorFlow API]tf.placeholder()用法</title>
    <link href="http://www.secret114.com/2019/07/04/TensorFlow-API-tf-placeholder-%E7%94%A8%E6%B3%95/"/>
    <id>http://www.secret114.com/2019/07/04/TensorFlow-API-tf-placeholder-%E7%94%A8%E6%B3%95/</id>
    <published>2019-07-04T03:02:43.000Z</published>
    <updated>2019-07-04T03:25:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>API地址：</p><h1 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h1><p> TensorFlow的设计理念称之为计算流图，在编写程序时，首先构筑整个系统的graph，代码并不会直接生效，这一点和python的其他数值计算库（如Numpy等）不同，graph为静态的，类似于docker中的镜像。然后，在实际的运行时，启动一个session，程序才会真正的运行。这样做的好处就是：避免反复地切换底层程序实际运行的上下文，TensorFlow帮你优化整个系统的代码。我们知道，很多python程序的底层为C语言或者其他语言，执行一行脚本，就要切换一次，是有成本的，TensorFlow通过计算流图的方式，帮你优化整个session需要执行的代码，还是很有优势的。</p><p>所以<code>placeholder()</code>函数是在神经网络构建graph的时候在模型中的占位，此时并没有把要输入的数据传入模型，它只会分配必要的内存。等建立session，在会话中，运行模型的时候通过<code>feed_dict()</code>函数向占位符喂入数据。</p><h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder(</span><br><span class="line">    dtype,</span><br><span class="line">    shape=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li><p>dtype: 数据类型，常用的是<code>float32</code>,<code>int64</code>等数值类型。</p></li><li><p>shape: 数据形状【默认None】</p></li><li><p>name：名字</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;API地址：&lt;/p&gt;
&lt;h1 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h1&gt;&lt;p&gt; TensorFlow的设计理念称之为计算流图，在编写程序时，首先构筑整个系统的graph，代码并不会直接生效，
      
    
    </summary>
    
    
    
      <category term="api" scheme="http://www.secret114.com/tags/api/"/>
    
  </entry>
  
  <entry>
    <title>[gensim API]word2vec用法</title>
    <link href="http://www.secret114.com/2019/07/02/gensim-API-word2vec%E7%94%A8%E6%B3%95/"/>
    <id>http://www.secret114.com/2019/07/02/gensim-API-word2vec%E7%94%A8%E6%B3%95/</id>
    <published>2019-07-02T08:01:56.000Z</published>
    <updated>2019-07-02T09:11:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>API地址：<a href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec" target="_blank" rel="noopener">https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec</a></p><h1 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h1><p>gensim是用于NLP的Python包，它封装了好多有用的模型。其中<code>gensim.models</code>中最常用的就是封装了C版本的word2vec。</p><h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">gensim</span>.<span class="title">models</span>.<span class="title">word2vec</span>.<span class="title">Word2Vec</span><span class="params">(sentences=None, corpus_file=None, size=<span class="number">100</span>, alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>, max_vocab_size=None, sample=<span class="number">0.001</span>, seed=<span class="number">1</span>, workers=<span class="number">3</span>, min_alpha=<span class="number">0.0001</span>, sg=<span class="number">0</span>, hs=<span class="number">0</span>, negative=<span class="number">5</span>, ns_exponent=<span class="number">0.75</span>, cbow_mean=<span class="number">1</span>, hashfxn=&lt;built-in function hash&gt;, iter=<span class="number">5</span>, null_word=<span class="number">0</span>, trim_rule=None, sorted_vocab=<span class="number">1</span>, batch_words=<span class="number">10000</span>, compute_loss=False, callbacks=<span class="params">()</span>, max_final_vocab=None)</span></span></span><br></pre></td></tr></table></figure><p>emm……参数有点多，这边做简单介绍：</p><ul><li>sentences: 需要分析的语料了，可以是一个列表，或者从文件中遍历读出（<code>word2vec.LineSentence(filename)</code>）</li><li>size: 词向量维度【默认100】</li><li>window: 词向量上下文最大距离，对于一般语料推荐[5;10]之间</li><li>sg: word2vec模型的选择，0为CBOW模型；1为Skip-Gram模型【默认0，CBOW模型】</li><li>hs: word2vec解法的选择，0为Negative Sampling；1为Hierarchical Softmax【默认0，Negative Sampling】</li><li>cbow_mean: 仅用作CBOW在做投影的时候，为0</li><li>min_count: 需要计算词向量的最小词频</li><li>iter: 随机梯度下降法中迭代的最大次数【默认5】</li><li>alpha: 随机梯度下降法中迭代的初始步长【默认0.025】</li><li>min_alpha: 在迭代过程中逐渐减小步长</li></ul><p><strong>训练好的词向量将会存储在<code>model.wv</code>的KeyedVectors实例中。</strong></p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>参考：<a href="https://www.cnblogs.com/jiangxinyang/p/10207273.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangxinyang/p/10207273.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;API地址：&lt;a href=&quot;https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https
      
    
    </summary>
    
    
    
      <category term="api" scheme="http://www.secret114.com/tags/api/"/>
    
  </entry>
  
  <entry>
    <title>[Pandas API]replace()用法</title>
    <link href="http://www.secret114.com/2019/07/02/Pandas-API-replace-%E7%94%A8%E6%B3%95/"/>
    <id>http://www.secret114.com/2019/07/02/Pandas-API-replace-%E7%94%A8%E6%B3%95/</id>
    <published>2019-07-02T07:37:12.000Z</published>
    <updated>2019-07-02T07:46:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>API地址：<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html?highlight=replace#pandas.DataFrame.replace" target="_blank" rel="noopener">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html?highlight=replace#pandas.DataFrame.replace</a></p><h1 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h1><p><code>replace()</code>函数是Pandas库中用来批量替换数据的函数。</p><h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrame.repalce(to_replace=<span class="literal">None</span>,value=<span class="literal">None</span>,inplace=<span class="literal">False</span>,limit=<span class="literal">None</span>,regex=<span class="literal">False</span>,method=<span class="string">'pad'</span>)</span><br></pre></td></tr></table></figure><ul><li>to_replace: 将要被替换的值，主要分为三类<ul><li>numeric, str, regex: 等于或匹配的值将会被替换</li><li>str, regex, numeric列表：替换的列表长度必须相同</li><li>字典</li></ul></li><li>value: 要替换的值</li><li>inplace: 若为True则就地修改</li><li>limit: 向前或向后填充的数量</li><li>regex: 正则表达式，与前面的to_replace连用</li><li>method: {‘pad’, ‘ffill’,’bfill’, None}，用前面或后面的值来替换</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;API地址：&lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html?highlight=replace#pandas.DataFram
      
    
    </summary>
    
    
    
      <category term="api" scheme="http://www.secret114.com/tags/api/"/>
    
  </entry>
  
  <entry>
    <title>[Python API]split()用法</title>
    <link href="http://www.secret114.com/2019/07/02/Python-API-split-%E7%94%A8%E6%B3%95/"/>
    <id>http://www.secret114.com/2019/07/02/Python-API-split-%E7%94%A8%E6%B3%95/</id>
    <published>2019-07-02T03:27:30.000Z</published>
    <updated>2019-07-02T04:01:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>API地址：<a href="https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split" target="_blank" rel="noopener">https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split</a></p><p><img src="http://wx2.sinaimg.cn/mw690/83f4f5acly1g4lc2isn1vj20s60n2juv.jpg" alt="split()函数官方描述"></p><h1 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h1><p>通过指定分隔符对字符串进行切片，如果参数<code>num</code>有指定值，则分隔<code>num+1</code>个子字符串</p><h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">str.split(sep=<span class="literal">None</span>, maxsplit=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><ul><li>sep: 分隔符，默认为所有的空字符，包括空格、换行(<code>\n</code>)、制表符(<code>\t</code>)等。</li><li>maxsplit: 分割次数。默认为<code>-1</code>, 即分隔所有。</li></ul><h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; str = <span class="string">"a,b,c"</span></span><br><span class="line">&gt;&gt;&gt; str.split()     <span class="comment"># 默认分割所有空字符</span></span><br><span class="line">[<span class="string">'a,b,c'</span>]</span><br><span class="line">&gt;&gt;&gt; str.split(<span class="string">'.'</span>)  <span class="comment"># 也可以指定</span></span><br><span class="line">[<span class="string">'a,b,c'</span>]</span><br><span class="line">&gt;&gt;&gt; str = <span class="string">"www.baidu.com"</span></span><br><span class="line">&gt;&gt;&gt; str.split(<span class="string">'.'</span>)</span><br><span class="line">[<span class="string">'www'</span>, <span class="string">'baidu'</span>, <span class="string">'com'</span>]</span><br><span class="line">&gt;&gt;&gt; str.split(<span class="string">'.'</span>,1) <span class="comment"># 可以指定最大分割次数</span></span><br><span class="line">[<span class="string">'www'</span>, <span class="string">'baidu.com'</span>]</span><br><span class="line">&gt;&gt;&gt; str.split(<span class="string">'.'</span>,maxsplit=1) <span class="comment"># 参数名maxsplit可以忽略也可以写上</span></span><br><span class="line">[<span class="string">'www'</span>, <span class="string">'baidu.com'</span>]</span><br></pre></td></tr></table></figure><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p><code>split()</code>函数常与<code>strip()</code>连用，用于分割字符串，具体用法请参照<a href="https://1141937908.github.io/2019/07/02/Python-API-strip-%E7%94%A8%E6%B3%95/" target="_blank" rel="noopener">strip()用法</a>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;API地址：&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.pyt
      
    
    </summary>
    
    
    
      <category term="api" scheme="http://www.secret114.com/tags/api/"/>
    
  </entry>
  
  <entry>
    <title>[Python API]strip()用法</title>
    <link href="http://www.secret114.com/2019/07/02/Python-API-strip-%E7%94%A8%E6%B3%95/"/>
    <id>http://www.secret114.com/2019/07/02/Python-API-strip-%E7%94%A8%E6%B3%95/</id>
    <published>2019-07-02T03:04:04.000Z</published>
    <updated>2019-07-02T04:00:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>API地址：<a href="https://docs.python.org/3/library/stdtypes.html?highlight=strip#str.strip" target="_blank" rel="noopener">https://docs.python.org/3/library/stdtypes.html?highlight=strip#str.strip</a></p><p><img src="http://wx2.sinaimg.cn/mw690/83f4f5acly1g4lbpbdrvaj20rz0cu76f.jpg" alt="strip()函数官方描述"></p><h1 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h1><p>用于移除字符串头尾指定的字符（默认为<code>空格</code>或<code>换行符</code>）。</p><p><strong>注意：该方法只能删除开头或结尾的字符，不能删除中间部分的字符。</strong></p><h1 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">str.strip([chars])</span><br></pre></td></tr></table></figure><ul><li>chars: 移除字符串头尾指定的字符序列</li></ul><h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; str = <span class="string">"          weibo            "</span></span><br><span class="line">&gt;&gt;&gt; str.strip()</span><br><span class="line"><span class="string">'weibo'</span></span><br><span class="line">&gt;&gt;&gt; str = <span class="string">"www.baidu.com"</span></span><br><span class="line">&gt;&gt;&gt; str.strip(<span class="string">'w.com'</span>)</span><br><span class="line"><span class="string">'baidu'</span></span><br></pre></td></tr></table></figure><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p><code>strip()</code>函数常与<code>split()</code>连用，用于分割字符串，具体用法请参照<a href="https://1141937908.github.io/2019/07/02/Python-API-split-用法/" target="_blank" rel="noopener">split()用法</a>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;API地址：&lt;a href=&quot;https://docs.python.org/3/library/stdtypes.html?highlight=strip#str.strip&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.pyt
      
    
    </summary>
    
    
    
      <category term="api" scheme="http://www.secret114.com/tags/api/"/>
    
  </entry>
  
  <entry>
    <title>NLP初学者如何查阅学术资料</title>
    <link href="http://www.secret114.com/2019/06/09/NLP%E5%88%9D%E5%AD%A6%E8%80%85%E5%A6%82%E4%BD%95%E6%9F%A5%E9%98%85%E5%AD%A6%E6%9C%AF%E8%B5%84%E6%96%99/"/>
    <id>http://www.secret114.com/2019/06/09/NLP%E5%88%9D%E5%AD%A6%E8%80%85%E5%A6%82%E4%BD%95%E6%9F%A5%E9%98%85%E5%AD%A6%E6%9C%AF%E8%B5%84%E6%96%99/</id>
    <published>2019-06-09T15:19:43.000Z</published>
    <updated>2019-06-09T16:14:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>读研接近一年以来，自己逐渐需要开始写论文，并往人工智能NLP相关领域内进行深入挖掘，只是之前各方面原因，未能有效地监督自己好好读论文并形成自己的体系，所以现在想趁着写论文的这段时间内做个笔记，算作是边学边做吧。</p><p>对于经验之谈，现在的我还处于菜鸟阶段，没有深入了解相关行业的动向，所以只能靠网上资料慢慢积累。知乎、简书、CSDN上都有相关介绍，所以我就在这里算是做个总结性的文章，把我看到的经验之谈搬运过来，作为自己在初学道路上的索引。</p><h1 id="1、国际学术组织、学术会议、学术论文"><a href="#1、国际学术组织、学术会议、学术论文" class="headerlink" title="1、国际学术组织、学术会议、学术论文"></a>1、国际学术组织、学术会议、学术论文</h1><p>自然语言处理（natural language processing，NLP）在很大程度上与计算语言学（computational linguistics，CL）重合。与其他计算机学科类似，NLP/CL有一个属于自己的最权威的国际专业学会，叫做The Association for Computational Linguistics（<a href="https://www.aclweb.org/portal/" target="_blank" rel="noopener" title="The Association for Computational Linguistics官网">ACL</a>）， 这个协会主办了NLP/CL领域最权威的国际会议，即<strong>ACL年会</strong>，ACL学会还会在北美和欧洲召开分年会，分别称为<strong>NAACL</strong>和<strong>EACL</strong>。除此之外，ACL学会下设多个特殊兴趣小组（special interest groups，SIGs），聚集了NLP/CL不同子领域的学者，性质类似一个大学校园的兴趣社团。其中比较有名的诸如SIGDAT（Linguistic data and corpus-based approaches to NLP）、SIGNLL（Natural Language Learning）等。</p><p>这些SIGs也会召开一些国际学术会议，其中比较有名的就是SIGDAT组织的EMNLP（Conference on Empirical Methods on Natural Language Processing）和SIGNLL组织的CoNLL（Conference on Natural Language Learning）。此外还有一个International Committee on Computational Linguistics的老牌NLP/CL学术组织，它每两年组织一个称为International Conference on Computational Linguistics (COLING)的国际会议，也是NLP/CL的重要学术会议。NLP/CL的主要学术论文就分布在这些会议上。</p><p>NLP/CL领域最大的好处在于，ACL学会建立了一个<a href="https://aclweb.org/anthology/" target="_blank" rel="noopener" title="ACL Anthology网站首页">ACL Anthology</a>的网站，支持该领域绝大部分国际学术会议论文的免费下载，甚至包含了其他组织主办的学术会议，例如COLING、IJCNLP等，并支持基于Google的全文检索功能，可谓一站在手，NLP论文我有。由于这个论文集合非常庞大，并且可以开放获取，很多学者也基于它开展研究，提供了更丰富的检索支持，具体入口可以参考ACL Anthology页面上方搜索框右侧的不同检索按钮。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.jianshu.com/p/db0a2a2cc2ba" target="_blank" rel="noopener">【简书】初学者如何查阅自然语言处理（NLP）领域学术资料</a></li><li><a href="https://www.aclweb.org/portal/" target="_blank" rel="noopener" title="The Association for Computational Linguistics官网">ACL学会官网</a></li><li><a href="https://blog.csdn.net/weixin_34613450/article/details/86679630" target="_blank" rel="noopener">【CSDN】NLP领域国内外知名会议和期刊</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;读研接近一年以来，自己逐渐需要开始写论文，并往人工智能NLP相关领域内进行深入挖掘，只是之前各方面原因，未能有效地监督自己好好读论文并形成自己的体系，所以现在想趁着写论文的这段时间内做个笔记，算作是边学边做吧。&lt;/p&gt;
&lt;p&gt;对于经验之谈，现在的我还处于菜鸟阶段，没有深入了
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
